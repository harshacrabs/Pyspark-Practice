{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3391decf-cc35-416b-85fa-181ee1b234e2",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca3b8c0-bdbf-4a03-8d5d-4008e1e6c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,max,min,count,approx_count_distinct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000ebb-af44-462f-908f-508aa16caafa",
   "metadata": {},
   "source": [
    "## Create a sparksession object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bb37f6-194a-4dcd-b9cd-adeecbbe14a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/16 13:01:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"pivot-table-pyspark\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512m\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1b6c2-4df9-45e9-bb56-db61339ecfa3",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5844558c-a09a-4107-b354-2068f4766ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- show_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- cast: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- listed_in: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark.read.format(\"csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"nullValue\", \"null\")\n",
    "      .option(\"dateFormat\", \"LLLL d, Y\")\n",
    "      .load(\"../../data/netflix_titles.csv\")\n",
    "     )\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ed06c-3cce-4e9e-b067-363a0224c685",
   "metadata": {},
   "source": [
    "## Create Pivot Table"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ded9c64-f5b2-4a5e-879f-a625f5c85339",
   "metadata": {},
   "source": [
    "We group the data frame by the \"country\" column. Then we apply pivot function to the \"type\" column. \n",
    "The distinct values of the type coumn will become the new columns in the resulting dataframe. \n",
    "Finally, we count the number of records using the count function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7acd2d-c6b2-410b-aca5-afdbdcfbb18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------+\n",
      "|             country|Movie|TV Show|\n",
      "+--------------------+-----+-------+\n",
      "|Peru, United Stat...|    1|   null|\n",
      "|United Kingdom, C...| null|      1|\n",
      "|India, United Kin...|    1|   null|\n",
      "|      India, Germany|    2|   null|\n",
      "|Japan, Canada, Un...| null|      1|\n",
      "|South Africa, Uni...|    1|   null|\n",
      "|              Russia|    1|     14|\n",
      "|United Kingdom, G...| null|      1|\n",
      "|Chile, United Sta...|    1|   null|\n",
      "|  Philippines, Qatar|    1|   null|\n",
      "|United States, Fr...| null|      1|\n",
      "|Hong Kong, China,...|    1|   null|\n",
      "|Denmark, France, ...|    1|   null|\n",
      "|South Africa, Angola|    1|   null|\n",
      "|United States, Po...| null|      1|\n",
      "|  Germany, Sri Lanka|    1|   null|\n",
      "|United Kingdom, N...|    2|   null|\n",
      "|Australia, United...|    2|   null|\n",
      "|United States, Ir...|    2|      1|\n",
      "|Brazil, France, G...|    1|   null|\n",
      "+--------------------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_table = df.groupBy(\"country\").pivot(\"type\").agg(count(\"show_id\"))\n",
    "\n",
    "pivot_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3581fce-b8c1-4770-892f-172df34dc24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
